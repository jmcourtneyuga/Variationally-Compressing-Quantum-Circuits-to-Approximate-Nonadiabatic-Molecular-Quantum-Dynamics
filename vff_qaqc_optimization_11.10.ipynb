{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213d256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bc2feb-e558-4316-8942-c146c9f9734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit_aer import Aer, AerSimulator\n",
    "from qiskit.quantum_info import state_fidelity, partial_trace, DensityMatrix, Statevector, Operator\n",
    "from vff_ansatz_8q_up import get_full_vff_quantum_circuit\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from qiskit.exceptions import QiskitError\n",
    "\n",
    "from CustomOperations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1e198-9e62-496f-a935-3c931a924d35",
   "metadata": {},
   "source": [
    "# Configuration, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d464b62-0c47-4011-86e9-ada372db6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # System parameters\n",
    "\n",
    "    trial = 4.0\n",
    "    QUBITS_NUM = 8             # Number of Position Register qubits\n",
    "    timestep = 10.0             # Trotter step size (smaller step = longer runtime, scales linearly)\n",
    "    diabaticity = 0.0         # Diabaticity parameter for potential energy\n",
    "    \n",
    "    # Physical constants\n",
    "    mass = 1818.18              # Mass term (in appropriate units)\n",
    "    box_size = 2               # Simulation box size\n",
    "    half_box = box_size / 2     # Half box size (box defined from -d to d)\n",
    "    hbar = 1.0                  # Reduced Planck constant (atomic units)\n",
    "    \n",
    "    # State space parameters\n",
    "    state_count = 2**QUBITS_NUM         # Number of discrete states in position basis\n",
    "    position_spacing = box_size / state_count  # Position grid spacing\n",
    "    momentum_spacing = 2*np.pi / (state_count * position_spacing)  # Momentum grid spacing\n",
    "    Nyquist = np.pi / position_spacing  # Nyquist frequency (max/min momentum)\n",
    "    \n",
    "    # Initial wave packet parameters\n",
    "    base_offset = 0       # Horizontal offset of initialized Gaussian wavepacket\n",
    "    momentum_init = -1.0         # Initial momentum of Gaussian wavepacket\n",
    "    wavepacket_width = 1.0/3.0  # Width of initialized Gaussian wavepacket\n",
    "    \n",
    "    # Potential parameters\n",
    "    potential_center = 0        # Relative center for simulation box\n",
    "    V1_strength = 0.015 # Strength of first quadratic potential (1/2 m*\\omega^2)\n",
    "    V2_strength = 0.015 # Strength of second quadratic potential (1/2 m*\\omega^2)\n",
    "    V1_offset = potential_center + 1.5  # Offset for first potential\n",
    "    V2_offset = potential_center - 1.5  # Offset for second potential\n",
    "    \n",
    "    # VFF optimization parameters\n",
    "    NUM_LAYERS_EIGENVECTOR = 2  # Number of eigenvector ansatz layers\n",
    "    NUM_LAYERS_DIAGONAL = 1    # Number of diagonal ansatz layers\n",
    "    MAX_ITERATIONS = 200       # Maximum optimization iterations\n",
    "    LEARNING_RATE = 5.0         # Initial learning rate\n",
    "    ADAPTIVE_LR = True          # Whether to use adaptive learning rate\n",
    "    LR_PATIENCE = 5             # Patience counter for learning rate adaptation\n",
    "    MIN_LR = 0.01             # Minimum learning rate\n",
    "    MAX_LR = 20.0               # Maximum learning rate\n",
    "    LR_FACTOR = 1.50             # Factor to increase/decrease learning rate\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    FAST_FORWARD_N_VALUES = [1, 2, 4, 8, 16, 32, 64, 128]  # Values of N to evaluate\n",
    "    \n",
    "    # Cached simulators for reuse\n",
    "    STATEVECTOR_SIMULATOR = Aer.get_backend('statevector_simulator')\n",
    "    UNITARY_SIMULATOR = Aer.get_backend('unitary_simulator')\n",
    "    \n",
    "    # File naming methods\n",
    "    @classmethod\n",
    "    def get_base_filename(cls):\n",
    "        current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        return f\"Trial{cls.trial}_{current_date}_{cls.QUBITS_NUM}q_vff_kinetic_timestep{cls.timestep}_x0{cls.base_offset}_p0{cls.momentum_init}_alpha{cls.diabaticity}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_optimizer_filename(cls):\n",
    "        return f\"{cls.get_base_filename()}_parameter_optimization.txt\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_params_filename(cls):\n",
    "        return f\"{cls.get_base_filename()}_optimized.txt\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_evaluation_filename(cls):\n",
    "        return f\"{cls.get_base_filename()}_fast_forwarding_evaluation.txt\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_plot_filename(cls):\n",
    "        return f\"{cls.get_base_filename()}_fidelity_plot.png\"\n",
    "    \n",
    "    @classmethod\n",
    "    def calculate_parameter_counts(cls):\n",
    "        w_params_per_layer = cls.QUBITS_NUM + (cls.QUBITS_NUM//2) + ((cls.QUBITS_NUM-1)//2)\n",
    "        total_w_params = (cls.NUM_LAYERS_EIGENVECTOR * w_params_per_layer)\n",
    "        d_params_per_layer = cls.QUBITS_NUM\n",
    "        total_d_params = cls.NUM_LAYERS_DIAGONAL * d_params_per_layer\n",
    "        return total_w_params, total_d_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b413981-096c-4266-966c-efb807ec204e",
   "metadata": {},
   "source": [
    "# Quantum Circuit Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439fb4f4-7e82-44aa-bdb1-e8bb72f5c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_zeroth_order_operations(angle, circuit, target):\n",
    "    circuit.p(angle, target)\n",
    "    circuit.x(target)\n",
    "    circuit.p(angle, target)\n",
    "    circuit.x(target)\n",
    "\n",
    "def apply_first_order_operations(angle, circuit, position_register):\n",
    "    for qubit in range(Config.QUBITS_NUM):\n",
    "        bit_order = qubit\n",
    "        position_scaling = 2**bit_order\n",
    "        target = position_register[bit_order]\n",
    "        circuit.p(angle * position_scaling, target)\n",
    "\n",
    "def apply_second_order_operations(angle, circuit, position_register):\n",
    "    for control in range(Config.QUBITS_NUM):\n",
    "        bit_order = 2*(control)\n",
    "        position_scaling = 2**(bit_order)\n",
    "        circuit.p(angle*position_scaling, position_register[control])\n",
    "        for target in range(Config.QUBITS_NUM):\n",
    "            if target != control:\n",
    "                bit_order = (control) + (target)\n",
    "                position_scaling = 2**(bit_order)\n",
    "                circuit.cp(angle*position_scaling, position_register[control], position_register[target])\n",
    "\n",
    "def apply_kinetic_term(circuit, position_register, timestep):\n",
    "    beta = (-Config.Nyquist - Config.momentum_init) / Config.momentum_spacing\n",
    "    #beta = (-Config.Nyquist) / Config.momentum_spacing\n",
    "    gamma = (Config.momentum_spacing)**2 / (2*Config.mass*Config.hbar)\n",
    "    \n",
    "    theta_1 = -(timestep * gamma * beta**2)\n",
    "    theta_2 = -2 * timestep * gamma * beta\n",
    "    theta_3 = -timestep * gamma\n",
    "    \n",
    "    # Apply quadratic phase\n",
    "    apply_zeroth_order_operations(theta_1, circuit, position_register[0])\n",
    "    \n",
    "    # Apply linear phase for momentum shift\n",
    "    apply_first_order_operations(theta_2, circuit, position_register)\n",
    "    \n",
    "    # Apply kinetic energy operator\n",
    "    apply_second_order_operations(theta_3, circuit, position_register)\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "def apply_harmonic_potential(circuit, position_register, timestep, potential_num):\n",
    "    if potential_num == 1:\n",
    "        vert_offset = Config.diabaticity\n",
    "        horiz_offset = Config.V1_offset\n",
    "        strength = Config.V1_strength\n",
    "    elif potential_num == 2: \n",
    "        vert_offset = 0.0\n",
    "        horiz_offset = Config.V2_offset\n",
    "        strength = Config.V2_strength\n",
    "\n",
    "    beta = (-Config.half_box - horiz_offset + (Config.position_spacing)/2) / Config.position_spacing\n",
    "    gamma = strength * (Config.position_spacing)**2 / Config.hbar\n",
    "    \n",
    "    theta_1 = -(gamma * beta**2 + vert_offset)\n",
    "    theta_2 = -2 * gamma * beta\n",
    "    theta_3 = -gamma\n",
    "\n",
    "    apply_zeroth_order_operations(theta_1, circuit, position_register[0])\n",
    "    apply_first_order_operations(theta_2, circuit, position_register)\n",
    "    apply_second_order_operations(theta_3, circuit, position_register)\n",
    "\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270144c6-891a-453b-846e-3d9edb6bd965",
   "metadata": {},
   "source": [
    "# Circuit Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711ff34d-6f21-49c9-be9f-9dd6a1e96e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target_unitary():\n",
    "    position_register = QuantumRegister(Config.QUBITS_NUM, name=\"position\")\n",
    "    qc = QuantumCircuit(position_register)  # Fixed typo: position_register -> position_register\n",
    "    \n",
    "    #cqft(qc, position_register, Config.QUBITS_NUM)\n",
    "    apply_kinetic_term(qc, position_register, Config.timestep)\n",
    "    #ciqft(qc, position_register, Config.QUBITS_NUM)\n",
    "    #apply_harmonic_potential(qc, position_register, Config.timestep, 1)\n",
    "    \n",
    "    return qc\n",
    "\n",
    "def build_vff_ansatz(params):\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    return get_full_vff_quantum_circuit(\n",
    "        params, \n",
    "        Config.NUM_LAYERS_EIGENVECTOR, \n",
    "        Config.NUM_LAYERS_DIAGONAL, \n",
    "        Config.QUBITS_NUM\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe351bf8-01ff-444e-afed-e084c29ed40e",
   "metadata": {},
   "source": [
    "# Fidelity, Cost Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa3c36f-b66e-4f2d-b664-f449877af775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lhst_cost(target_unitary, v_circuit):\n",
    "    fidelities = []\n",
    "    n_qubits = 8\n",
    "\n",
    "    target_matrix = Operator(target_unitary).data\n",
    "    v_matrix = Operator(v_circuit).data\n",
    "    \n",
    "    # Pre-compute these outside the loop\n",
    "    u_dag = target_matrix.conj().T\n",
    "    v_dag = v_matrix.conj().T\n",
    "    u_full = np.kron(u_dag, np.eye(2**n_qubits))\n",
    "    v_full = np.kron(np.eye(2**n_qubits), v_dag)\n",
    "    \n",
    "    # Create Bell state once\n",
    "    bell_state = np.array([1, 0, 0, 1]) / np.sqrt(2)\n",
    "    \n",
    "    for j in range(n_qubits):\n",
    "        # Create full system Bell state for qubits A_j and B_j\n",
    "        full_dim = 2**(n_qubits*2)\n",
    "        full_state = np.zeros(full_dim, dtype=complex)\n",
    "        \n",
    "        # Place Bell state at positions j and j+n_qubits\n",
    "        for i in range(full_dim):\n",
    "            # Extract bit j from A register and bit j from B register\n",
    "            bit_a_j = (i >> j) & 1\n",
    "            bit_b_j = (i >> (j + n_qubits)) & 1\n",
    "            \n",
    "            # Check if in Bell state configuration\n",
    "            if (bit_a_j == 0 and bit_b_j == 0) or (bit_a_j == 1 and bit_b_j == 1):\n",
    "                full_state[i] = bell_state[bit_a_j * 2 + bit_b_j]\n",
    "        \n",
    "        # Normalize\n",
    "        full_state = full_state / np.linalg.norm(full_state)\n",
    "        \n",
    "        # Create the quantum channel E_j as per equation S3\n",
    "        # First apply U^dagger to qubit A_j and its environment\n",
    "        u_dag = target_matrix.conj().T\n",
    "        u_full = np.kron(u_dag, np.eye(2**n_qubits))  # Act on register A\n",
    "        \n",
    "        # Apply V^dagger to qubit B_j and its environment  \n",
    "        v_dag = v_matrix.conj().T\n",
    "        v_full = np.kron(np.eye(2**n_qubits), v_dag)  # Act on register B\n",
    "        \n",
    "        # Apply U^dagger to register A and V^dagger to register B\n",
    "        state_u = u_full @ full_state\n",
    "        state_v = v_full @ full_state\n",
    "        \n",
    "        # Compute the quantum channel effects\n",
    "        # Trace out all qubits except j in A and j in B\n",
    "        qubits_to_trace = [q for q in range(n_qubits*2) if q != j and q != j + n_qubits]\n",
    "        \n",
    "        # Get reduced density matrices\n",
    "        dm_u = partial_trace(DensityMatrix(state_u), qubits_to_trace)\n",
    "        dm_v = partial_trace(DensityMatrix(state_v), qubits_to_trace)\n",
    "        \n",
    "        # Create Bell density matrix for comparison\n",
    "        bell_dm = DensityMatrix(bell_state)\n",
    "        \n",
    "        # Calculate entanglement fidelity F_e^(j)\n",
    "        f_e = state_fidelity(dm_u, dm_v)\n",
    "        fidelities.append(f_e)\n",
    "    \n",
    "    # Calculate cost according to equation S1\n",
    "    cost = 1.0 - (1.0/n_qubits) * np.sum(fidelities)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb958e9-9f50-4dc8-924a-648bb646ead4",
   "metadata": {},
   "source": [
    "# Gradient Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bfdb18-01de-499a-bee5-1a4d2fbe108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_w_gradient(params, param_idx, target_unitary):\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    \n",
    "    # Ensure we're calculating W gradient\n",
    "    assert param_idx < total_w_params\n",
    "    \n",
    "    # Term 1: C_LHST(U, W_+^k DW^dagger)\n",
    "    params_plus_k = params.copy()\n",
    "    params_plus_k[param_idx] += np.pi/2\n",
    "    v1 = get_full_vff_quantum_circuit(\n",
    "        params_plus_k,\n",
    "        Config.NUM_LAYERS_EIGENVECTOR,\n",
    "        Config.NUM_LAYERS_DIAGONAL,\n",
    "        Config.QUBITS_NUM\n",
    "    )\n",
    "    cost1 = calculate_lhst_cost(target_unitary, v1)\n",
    "    \n",
    "    # Term 2: C_LHST(U, W_-^k DW^dagger)\n",
    "    params_minus_k = params.copy()\n",
    "    params_minus_k[param_idx] -= np.pi/2\n",
    "    v2 = get_full_vff_quantum_circuit(\n",
    "        params_minus_k,\n",
    "        Config.NUM_LAYERS_EIGENVECTOR,\n",
    "        Config.NUM_LAYERS_DIAGONAL,\n",
    "        Config.QUBITS_NUM\n",
    "    )\n",
    "    cost2 = calculate_lhst_cost(target_unitary, v2)\n",
    "    \n",
    "    # Term 3: C_LHST(U, WD(W_+^k)^dagger)\n",
    "    v3 = get_full_vff_quantum_circuit(\n",
    "        params,\n",
    "        Config.NUM_LAYERS_EIGENVECTOR,\n",
    "        Config.NUM_LAYERS_DIAGONAL,\n",
    "        Config.QUBITS_NUM,\n",
    "        w_shift_type='O+',\n",
    "        shift_param_idx=param_idx\n",
    "    )\n",
    "    cost3 = calculate_lhst_cost(target_unitary, v3)\n",
    "    \n",
    "    # Term 4: C_LHST(U, WD(W_-^k)^dagger)\n",
    "    v4 = get_full_vff_quantum_circuit(\n",
    "        params,\n",
    "        Config.NUM_LAYERS_EIGENVECTOR,\n",
    "        Config.NUM_LAYERS_DIAGONAL,\n",
    "        Config.QUBITS_NUM,\n",
    "        w_shift_type='O-',\n",
    "        shift_param_idx=param_idx\n",
    "    )\n",
    "    cost4 = calculate_lhst_cost(target_unitary, v4)\n",
    "    \n",
    "    # Calculate gradient according to equation 27\n",
    "    gradient = 0.5 * ((cost1 - cost2) + (cost3 - cost4))\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "def calculate_d_gradient(params, param_idx, target_unitary):\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    assert param_idx >= total_w_params\n",
    "    \n",
    "    # Term 1: C_LHST(U, WD_+^l W^dagger)\n",
    "    params_plus_l = params.copy()\n",
    "    params_plus_l[param_idx] += np.pi/2\n",
    "    v1 = get_full_vff_quantum_circuit(\n",
    "        params_plus_l,\n",
    "        Config.NUM_LAYERS_EIGENVECTOR,\n",
    "        Config.NUM_LAYERS_DIAGONAL,\n",
    "        Config.QUBITS_NUM\n",
    "    )\n",
    "    cost1 = calculate_lhst_cost(target_unitary, v1)\n",
    "    \n",
    "    # Term 2: C_LHST(U, WD_-^l W^dagger)\n",
    "    params_minus_l = params.copy()\n",
    "    params_minus_l[param_idx] -= np.pi/2\n",
    "    v2 = get_full_vff_quantum_circuit(\n",
    "        params_minus_l,\n",
    "        Config.NUM_LAYERS_EIGENVECTOR,\n",
    "        Config.NUM_LAYERS_DIAGONAL,\n",
    "        Config.QUBITS_NUM\n",
    "    )\n",
    "    cost2 = calculate_lhst_cost(target_unitary, v2)\n",
    "    \n",
    "    # Calculate gradient according to equation 29\n",
    "    gradient = 0.5 * (cost1 - cost2)\n",
    "    \n",
    "    return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f6c342-3519-435b-b19e-9c0ecf0b8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_wrapper(args):\n",
    "    param_index, params, target_unitary = args\n",
    "    try:\n",
    "        total_w_params, _ = Config.calculate_parameter_counts()\n",
    "        \n",
    "        if param_index < total_w_params:\n",
    "            gradient = calculate_w_gradient(params, param_index, target_unitary)\n",
    "        else:\n",
    "            gradient = calculate_d_gradient(params, param_index, target_unitary)\n",
    "            \n",
    "        return (param_index, gradient)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating gradient for parameter {param_index}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return (param_index, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5433649f-4897-4819-94e2-b0416fc9b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients_parallel(params, target_unitary, param_indices):\n",
    "    num_workers = 2\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    \n",
    "    # Consider sequential processing instead:\n",
    "    gradients = np.zeros_like(params)\n",
    "    for param_index in param_indices:\n",
    "        if param_index < total_w_params:\n",
    "            gradients[param_index] = calculate_w_gradient(params, param_index, target_unitary)\n",
    "        else:\n",
    "            gradients[param_index] = calculate_d_gradient(params, param_index, target_unitary)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218cb04-7aec-4aae-a38e-8c465ec3ca04",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c54f25-5d6c-4f02-beea-45d9e18fa871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_gradient_calculation(params, param_index, target_unitary):\n",
    "    total_w_params, _ = Config.calculate_parameter_counts()\n",
    "    \n",
    "    # Calculate analytical gradient\n",
    "    if param_index < total_w_params:\n",
    "        analytical_gradient = calculate_w_gradient(params, param_index, target_unitary)\n",
    "    else:\n",
    "        analytical_gradient = calculate_d_gradient(params, param_index, target_unitary)\n",
    "    \n",
    "    # Calculate numerical gradient (finite difference)\n",
    "    epsilon = 0.01\n",
    "    \n",
    "    # Calculate original cost\n",
    "    vff_ansatz = build_vff_ansatz(params)\n",
    "    original_cost = calculate_lhst_cost(target_unitary, vff_ansatz)\n",
    "    \n",
    "    # Perturb parameter\n",
    "    params_plus = params.copy()\n",
    "    params_plus[param_index] += epsilon\n",
    "    vff_ansatz_plus = build_vff_ansatz(params_plus)\n",
    "    cost_plus = calculate_lhst_cost(target_unitary, vff_ansatz_plus)\n",
    "    \n",
    "    params_minus = params.copy()\n",
    "    params_minus[param_index] -= epsilon\n",
    "    vff_ansatz_minus = build_vff_ansatz(params_minus)\n",
    "    cost_minus = calculate_lhst_cost(target_unitary, vff_ansatz_minus)\n",
    "    \n",
    "    # Central difference\n",
    "    numerical_gradient = (cost_plus - cost_minus) / (2 * epsilon)\n",
    "    \n",
    "    print(f\"Parameter {param_index}:\")\n",
    "    print(f\"  Analytical gradient: {analytical_gradient}\")\n",
    "    print(f\"  Numerical gradient: {numerical_gradient}\")\n",
    "    print(f\"  Difference: {abs(analytical_gradient - numerical_gradient)}\")\n",
    "    \n",
    "    return analytical_gradient, numerical_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fed2b7-56fc-43ab-aaa2-71b1749863c7",
   "metadata": {},
   "source": [
    "# Fast-Forwarding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d05c04-cf47-4c64-8474-fc798e8d02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_forward(optimized_params, target_unitary, n):\n",
    "    # Get parameter counts\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    \n",
    "    # Create the fast-forwarded parameters - scale only diagonal parameters by n\n",
    "    ff_params = optimized_params.copy()\n",
    "    for i in range(total_w_params, total_w_params + total_d_params):\n",
    "        ff_params[i] = optimized_params[i] * n\n",
    "    \n",
    "    # Build the fast-forwarded VFF ansatz\n",
    "    vff_ff_ansatz = build_vff_ansatz(ff_params)\n",
    "    \n",
    "    # Create the target unitary raised to power n\n",
    "    target_n_circuit = QuantumCircuit(Config.QUBITS_NUM)\n",
    "    for _ in range(n):\n",
    "        target_n_circuit.compose(target_unitary, inplace=True)\n",
    "    \n",
    "    # Calculate the cost\n",
    "    cost = calculate_lhst_cost(target_n_circuit, vff_ff_ansatz)\n",
    "    fidelity = 1.0 - cost\n",
    "    \n",
    "    return fidelity\n",
    "\n",
    "def parallel_fast_forward(args):\n",
    "    n, optimized_params, target_unitary = args\n",
    "    fidelity = fast_forward(optimized_params, target_unitary, n)\n",
    "    return n, fidelity\n",
    "\n",
    "def evaluate_fast_forwarding(optimized_params, target_unitary, n_values=None):\n",
    "    if n_values is None:\n",
    "        n_values = Config.FAST_FORWARD_N_VALUES\n",
    "        \n",
    "    arg_tuples = [(n, optimized_params, target_unitary) for n in n_values]\n",
    "    \n",
    "    results = []\n",
    "    evaluation_filename = Config.get_evaluation_filename()\n",
    "    \n",
    "    # Create empty evaluation file\n",
    "    with open(evaluation_filename, 'w') as f:\n",
    "        f.write(\"N\\tFidelity\\n\")\n",
    "    \n",
    "    # Determine number of workers based on CPU count and number of n values\n",
    "    num_workers = min(os.cpu_count(), len(n_values))\n",
    "    \n",
    "    # Run evaluations in parallel\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for n, fidelity in tqdm(\n",
    "            executor.map(parallel_fast_forward, arg_tuples),\n",
    "            total=len(n_values),\n",
    "            desc=\"Evaluating fast-forwarding\"\n",
    "        ):\n",
    "            results.append((n, fidelity))\n",
    "            \n",
    "            # Write to file\n",
    "            with open(evaluation_filename, 'a') as f:\n",
    "                f.write(f\"{n}\\t{fidelity}\\n\")\n",
    "    \n",
    "    # Sort results by n for consistency\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_fast_forwarding_results(results):\n",
    "    n_values = [r[0] for r in results]\n",
    "    fidelities = [r[1] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(n_values, fidelities, 'o-', linewidth=2)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Number of Time Steps (N)')\n",
    "    plt.ylabel('Fidelity')\n",
    "    plt.title(f'VFF Fast-Forwarding Performance ({Config.QUBITS_NUM} qubits)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plot_filename = Config.get_plot_filename()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Fidelity plot saved to {plot_filename}\")\n",
    "    \n",
    "    return plot_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aac883-4c34-4d86-82f8-7924544a908b",
   "metadata": {},
   "source": [
    "# Main Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd38e49a-a980-42f3-a838-ddf9b14c0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_circuit_visualization(params=None):\n",
    "    # Get parameter counts\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    total_params = total_w_params + total_d_params\n",
    "    \n",
    "    # Generate random parameters if none provided\n",
    "    if params is None:\n",
    "        params = np.random.random(total_params) * 2 * np.pi\n",
    "        print(\"Using random parameters for circuit visualization\")\n",
    "    \n",
    "    # Build the VFF ansatz circuit\n",
    "    circuit = build_vff_ansatz(params)\n",
    "    \n",
    "    # Print circuit information\n",
    "    print(f\"VFF Ansatz Circuit Details:\")\n",
    "    print(f\"- Number of qubits: {Config.QUBITS_NUM}\")\n",
    "    print(f\"- Eigenvector layers: {Config.NUM_LAYERS_EIGENVECTOR}\")\n",
    "    print(f\"- Diagonal layers: {Config.NUM_LAYERS_DIAGONAL}\")\n",
    "    print(f\"- Total parameters: {total_params} ({total_w_params} W params, {total_d_params} D params)\")\n",
    "    \n",
    "    print(\"\\n Circuit Representation:\")\n",
    "    print(circuit.draw(output='text', fold=120))\n",
    "    \n",
    "    try:\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        circuit_drawing = circuit.draw(output='mpl')\n",
    "        plt.title(f\"VFF Ansatz ({Config.QUBITS_NUM} qubits, {Config.NUM_LAYERS_EIGENVECTOR}W/{Config.NUM_LAYERS_DIAGONAL}D layers)\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the circuit drawing to a file\n",
    "        circuit_filename = f\"{Config.get_base_filename()}_circuit.png\"\n",
    "        plt.savefig(circuit_filename)\n",
    "        plt.close()\n",
    "        print(f\"\\nCircuit visualization saved to {circuit_filename}\")\n",
    "    except ImportError:\n",
    "        print(\"\\nMatplotlib not available for visual circuit rendering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c1b954-1ea6-42b2-aa39-1dc4351a230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(cached_u_states, params, param_indices):\n",
    "    print(\"\\nGradient Analysis:\")\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradient_terms = calculate_gradients_parallel(cached_u_states, params, param_indices)\n",
    "    \n",
    "    # Combine gradients\n",
    "    gradients = np.zeros_like(params)\n",
    "    for j in range(Config.QUBITS_NUM):\n",
    "        for k in param_indices:\n",
    "            key = (j, k)\n",
    "            if key in gradient_terms:\n",
    "                gradients[k] += gradient_terms[key] / Config.QUBITS_NUM\n",
    "    \n",
    "    # Gradient statistics\n",
    "    grad_norm = np.linalg.norm(gradients)\n",
    "    grad_mean = np.mean(gradients)\n",
    "    grad_std = np.std(gradients)\n",
    "    grad_min = np.min(gradients)\n",
    "    grad_max = np.max(gradients)\n",
    "    grad_nonzero = np.count_nonzero(gradients)\n",
    "    \n",
    "    print(f\"Gradient norm: {grad_norm:.6f}\")\n",
    "    print(f\"Gradient mean: {grad_mean:.6f}\")\n",
    "    print(f\"Gradient std dev: {grad_std:.6f}\")\n",
    "    print(f\"Gradient min/max: {grad_min:.6f}/{grad_max:.6f}\")\n",
    "    print(f\"Non-zero gradients: {grad_nonzero}/{len(gradients)} ({grad_nonzero/len(gradients)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for vanishing/exploding gradients\n",
    "    if grad_norm < 1e-4:\n",
    "        print(\"Possible vanishing gradients detected\")\n",
    "    if grad_norm > 10:\n",
    "        print(\"Possible exploding gradients detected\")\n",
    "    \n",
    "    # Parameter-wise gradient statistics\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    \n",
    "    w_grads = gradients[:total_w_params]\n",
    "    d_grads = gradients[total_w_params:]\n",
    "    \n",
    "    print(f\"\\nW-parameter gradients: mean={np.mean(w_grads):.6f}, std={np.std(w_grads):.6f}, norm={np.linalg.norm(w_grads):.6f}\")\n",
    "    print(f\"D-parameter gradients: mean={np.mean(d_grads):.6f}, std={np.std(d_grads):.6f}, norm={np.linalg.norm(d_grads):.6f}\")\n",
    "    \n",
    "    # Verify a few gradients with numerical method\n",
    "    print(\"\\nVerifying selected gradients numerically:\")\n",
    "    # Check a few W and D parameters\n",
    "    w_indices = list(range(0, total_w_params, total_w_params//min(3, total_w_params)))\n",
    "    d_indices = list(range(total_w_params, total_w_params + total_d_params, max(1, total_d_params//min(3, total_d_params))))\n",
    "    \n",
    "    for idx in w_indices + d_indices:\n",
    "        if idx < len(params):\n",
    "            analytical, numerical = verify_gradient_calculation(cached_u_states, params, idx)\n",
    "            if abs(numerical) > 1e-6:  # Avoid division by zero\n",
    "                ratio = analytical / numerical\n",
    "                print(f\"Parameter {idx}: analytical={analytical:.6f}, numerical={numerical:.6f}, ratio={ratio:.6f}\")\n",
    "            else:\n",
    "                print(f\"Parameter {idx}: analytical={analytical:.6f}, numerical={numerical:.6f} (ratio undefined)\")\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a283a3-2904-411b-9651-d8530b3468ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VFF workflow with 3 qubits and timestep=10.0\n",
      "Building target unitary...\n",
      "Skipping quasi-random search, starting optimization with random parameters...\n",
      "Initial cost: 0.86062196\n",
      "\n",
      "Starting gradient optimization...\n",
      "Starting optimization loop...\n",
      "Iteration 0: New best cost = 0.86062196 (improvement: inf)\n",
      "  Gradient norm: 0.165484\n",
      "Iteration 1: New best cost = 0.72083929 (improvement: 0.13978267)\n",
      "  Gradient norm: 0.159697\n",
      "Iteration 2: New best cost = 0.61263362 (improvement: 0.10820567)\n",
      "  Gradient norm: 0.133455\n",
      "Iteration 3: New best cost = 0.51151493 (improvement: 0.10111869)\n",
      "  Gradient norm: 0.172790\n",
      "Iteration 4: New best cost = 0.35928762 (improvement: 0.15222730)\n",
      "  Gradient norm: 0.172022\n",
      "Iteration 5: New best cost = 0.22240651 (improvement: 0.13688112)\n",
      "  Gradient norm: 0.163537\n",
      "Iteration 6: New best cost = 0.09267157 (improvement: 0.12973494)\n",
      "  Gradient norm: 0.141569\n",
      "Iteration 7: New best cost = 0.01995906 (improvement: 0.07271251)\n",
      "  Gradient norm: 0.055332\n",
      "Iteration 8: New best cost = 0.01065664 (improvement: 0.00930241)\n",
      "  Gradient norm: 0.011488\n",
      "Iteration 9: New best cost = 0.01026448 (improvement: 0.00039216)\n",
      "  Gradient norm: 0.002163\n",
      "Iteration 10: New best cost = 0.01025059 (improvement: 0.00001389)\n",
      "  Gradient norm: 0.000405\n",
      "  Reducing learning rate to 4.750000\n",
      "Iteration 11: New best cost = 0.01025010 (improvement: 0.00000049)\n",
      "  Gradient norm: 0.000076\n",
      "Iteration 12: New best cost = 0.01025009 (improvement: 0.00000002)\n",
      "  Gradient norm: 0.000017\n",
      "Iteration 13: New best cost = 0.01025008 (improvement: 0.00000000)\n",
      "  Gradient norm: 0.000004\n",
      "Iteration 14: Cost = 0.01025008\n",
      "  Gradient norm: 0.000001\n",
      "Iteration 15: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 16: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 17: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 18: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 19: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 20: Cost = 0.01025009\n",
      "  Gradient norm: 0.000000\n",
      "  Reducing learning rate to 4.512500\n",
      "Iteration 21: Cost = 0.01025009\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 22: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 23: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 24: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 25: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 26: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 27: New best cost = 0.01025008 (improvement: 0.00000000)\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 28: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 29: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 30: Cost = 0.01025009\n",
      "  Gradient norm: 0.000000\n",
      "  Reducing learning rate to 4.286875\n",
      "Iteration 31: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 32: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 33: Cost = 0.01025009\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 34: Cost = 0.01025009\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 35: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 36: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 37: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 38: Cost = 0.01025009\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 39: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 40: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "  Reducing learning rate to 4.072531\n",
      "Iteration 41: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 42: Cost = 0.01025009\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 43: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 44: Cost = 0.01025008\n",
      "  Gradient norm: 0.000000\n",
      "Iteration 45: Cost = 0.01025009\n"
     ]
    }
   ],
   "source": [
    "def run_vff_optimization():\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    total_params = total_w_params + total_d_params\n",
    "    \n",
    "    # Build target unitary (kinetic evolution)\n",
    "    target_unitary = build_target_unitary()\n",
    "    \n",
    "    # Initialize parameters\n",
    "    params = np.random.random(total_params) * 2 * np.pi\n",
    "    \n",
    "    # Optimization loop\n",
    "    learning_rate = 5.0\n",
    "    best_cost = float('inf')\n",
    "    best_params = params.copy()\n",
    "    \n",
    "    # Create optimization log file\n",
    "    with open(Config.get_optimizer_filename(), 'w') as f:\n",
    "        f.write(\"Iteration\\tCost\\tLearning_Rate\\n\")\n",
    "    \n",
    "    for iteration in range(Config.MAX_ITERATIONS):\n",
    "        # Calculate current cost\n",
    "        v_circuit = build_vff_ansatz(params)\n",
    "        cost = calculate_lhst_cost(target_unitary, v_circuit)\n",
    "        \n",
    "        # Update best parameters\n",
    "        if cost < best_cost:\n",
    "            best_cost = cost\n",
    "            best_params = params.copy()\n",
    "            print(f\"Iteration {iteration}: New best cost = {best_cost:.8f}\")\n",
    "        \n",
    "        # Calculate gradients\n",
    "        param_indices = list(range(total_params))\n",
    "        gradients = calculate_gradients_parallel(params, target_unitary, param_indices)\n",
    "        \n",
    "        # Update parameters according to equations 31-32\n",
    "        params = params - learning_rate * gradients\n",
    "        params = np.mod(params, 2*np.pi)  # Keep in [0, 2Ï€)\n",
    "        \n",
    "        # Log progress\n",
    "        with open(Config.get_optimizer_filename(), 'a') as f:\n",
    "            f.write(f\"{iteration}\\t{cost:.8f}\\t{learning_rate:.6f}\\n\")\n",
    "        \n",
    "        # Adaptive learning rate\n",
    "        if iteration % 10 == 0:\n",
    "            learning_rate *= 0.95  # Gradually reduce learning rate\n",
    "    \n",
    "    # Save best parameters\n",
    "    np.savetxt(Config.get_params_filename(), best_params)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "def quasi_random_parameter_search(target_unitary, num_samples=1000, top_k=5):\n",
    "    print(f\"Starting quasi-random search with {num_samples} samples...\")\n",
    "    \n",
    "    # Get parameter counts\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    total_params = total_w_params + total_d_params\n",
    "    \n",
    "    # Create search log file\n",
    "    random_search_filename = f\"{Config.get_base_filename()}_random_search.txt\"\n",
    "    with open(random_search_filename, 'w') as f:\n",
    "        f.write(\"Sample\\tCost\\n\")\n",
    "    \n",
    "    best_params = []\n",
    "    \n",
    "    # Use Sobol sequence for quasi-random sampling if available\n",
    "    try:\n",
    "        from scipy.stats import qmc\n",
    "        sampler = qmc.Sobol(d=total_params, scramble=True)\n",
    "        samples = sampler.random(num_samples) * 2 * np.pi\n",
    "        print(\"Using Sobol sequence for quasi-random sampling\")\n",
    "    except ImportError:\n",
    "        samples = np.random.random((num_samples, total_params)) * 2 * np.pi\n",
    "        print(\"Using standard random sampling\")\n",
    "    \n",
    "    # Evaluate all parameter sets\n",
    "    for i, params in enumerate(tqdm(samples, desc=\"Evaluating random parameters\")):\n",
    "        # Build VFF ansatz with these parameters\n",
    "        vff_ansatz = build_vff_ansatz(params)\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = calculate_lhst_cost(target_unitary, vff_ansatz)\n",
    "        \n",
    "        # Log this sample\n",
    "        with open(random_search_filename, 'a') as f:\n",
    "            f.write(f\"{i}\\t{cost:.8f}\\n\")\n",
    "        \n",
    "        # Update best parameters if needed\n",
    "        if len(best_params) < top_k:\n",
    "            best_params.append((params, cost))\n",
    "            best_params.sort(key=lambda x: x[1])  # Sort by cost\n",
    "        elif cost < best_params[-1][1]:\n",
    "            best_params[-1] = (params, cost)\n",
    "            best_params.sort(key=lambda x: x[1])\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Evaluated {i+1}/{num_samples}, current best cost: {best_params[0][1]:.8f}\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} parameter sets:\")\n",
    "    for i, (params, cost) in enumerate(best_params):\n",
    "        print(f\"Set {i+1}: Cost = {cost:.8f}\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "def optimize_multiple_parameter_sets(target_unitary, initial_param_sets):\n",
    "    results = []\n",
    "    \n",
    "    for i, (params, initial_cost) in enumerate(initial_param_sets):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Optimizing parameter set {i+1}/{len(initial_param_sets)}\")\n",
    "        print(f\"Initial cost: {initial_cost:.8f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create set-specific filenames\n",
    "        set_params_filename = f\"{Config.get_base_filename()}_set{i+1}_optimized.txt\"\n",
    "        \n",
    "        # Custom optimization with adaptive learning rate\n",
    "        current_params = params.copy()\n",
    "        best_cost = initial_cost\n",
    "        best_params = current_params.copy()\n",
    "        \n",
    "        # Optimization parameters\n",
    "        learning_rate = 5.0\n",
    "        min_lr = 0.01\n",
    "        patience = 10\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "        total_params = total_w_params + total_d_params\n",
    "        \n",
    "        for iteration in range(Config.MAX_ITERATIONS):\n",
    "            # Calculate current cost\n",
    "            v_circuit = build_vff_ansatz(current_params)\n",
    "            cost = calculate_lhst_cost(target_unitary, v_circuit)\n",
    "            \n",
    "            # Update best parameters\n",
    "            if cost < best_cost:\n",
    "                improvement = best_cost - cost\n",
    "                best_cost = cost\n",
    "                best_params = current_params.copy()\n",
    "                print(f\"Iteration {iteration}: New best cost = {best_cost:.8f} (improvement: {improvement:.8f})\")\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = np.zeros_like(current_params)\n",
    "            \n",
    "            # W parameters\n",
    "            for idx in range(total_w_params):\n",
    "                gradients[idx] = calculate_w_gradient(current_params, idx, target_unitary)\n",
    "            \n",
    "            # D parameters\n",
    "            for idx in range(total_w_params, total_params):\n",
    "                gradients[idx] = calculate_d_gradient(current_params, idx, target_unitary)\n",
    "            \n",
    "            # Check gradient norm\n",
    "            grad_norm = np.linalg.norm(gradients)\n",
    "            if grad_norm < 1e-8:\n",
    "                print(f\"Small gradient norm ({grad_norm:.2e}), optimization likely converged\")\n",
    "                break\n",
    "            \n",
    "            # Update parameters\n",
    "            current_params = current_params - learning_rate * gradients\n",
    "            current_params = np.mod(current_params, 2*np.pi)\n",
    "            \n",
    "            # Adaptive learning rate\n",
    "            if no_improvement_count >= patience:\n",
    "                learning_rate = max(learning_rate * 0.5, min_lr)\n",
    "                print(f\"Reducing learning rate to {learning_rate:.6f}\")\n",
    "                no_improvement_count = 0\n",
    "            \n",
    "            if learning_rate <= min_lr and no_improvement_count >= patience * 2:\n",
    "                print(\"Learning rate at minimum and no improvement, stopping\")\n",
    "                break\n",
    "        \n",
    "        # Save best parameters for this set\n",
    "        np.savetxt(set_params_filename, best_params)\n",
    "        results.append((best_params, best_cost))\n",
    "        print(f\"Set {i+1} final cost: {best_cost:.8f}\")\n",
    "    \n",
    "    # Sort results by final cost\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_fast_forwarding_evaluation(optimized_params, target_unitary):\n",
    "    print(f\"Evaluating fast-forwarding for N={Config.FAST_FORWARD_N_VALUES}\")\n",
    "    \n",
    "    results = []\n",
    "    evaluation_filename = Config.get_evaluation_filename()\n",
    "    \n",
    "    # Create empty evaluation file\n",
    "    with open(evaluation_filename, 'w') as f:\n",
    "        f.write(\"N\\tFidelity\\n\")\n",
    "    \n",
    "    # Run evaluations\n",
    "    for n in Config.FAST_FORWARD_N_VALUES:\n",
    "        fidelity = fast_forward(optimized_params, target_unitary, n)\n",
    "        results.append((n, fidelity))\n",
    "        \n",
    "        # Write to file\n",
    "        with open(evaluation_filename, 'a') as f:\n",
    "            f.write(f\"{n}\\t{fidelity:.8f}\\n\")\n",
    "        \n",
    "        print(f\"N={n}: Fidelity={fidelity:.8f}\")\n",
    "    \n",
    "    # Plot the results\n",
    "    plot_fast_forwarding_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_multi_init_optimization():\n",
    "    total_start = datetime.datetime.now()\n",
    "    print(f\"Starting VFF workflow with {Config.QUBITS_NUM} qubits and timestep={Config.timestep}\")\n",
    "    \n",
    "    # Build the target unitary\n",
    "    print(\"Building target unitary...\")\n",
    "    target_unitary = build_target_unitary()\n",
    "    \n",
    "    # Skip quasi-random search and go directly to optimization with random initial parameters\n",
    "    print(f\"Skipping quasi-random search, starting optimization with random parameters...\")\n",
    "    \n",
    "    # Initialize a single set of random parameters\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    total_params = total_w_params + total_d_params\n",
    "    initial_params = np.random.random(total_params) * 2 * np.pi\n",
    "    \n",
    "    # Calculate initial cost\n",
    "    vff_ansatz = build_vff_ansatz(initial_params)\n",
    "    initial_cost = calculate_lhst_cost(target_unitary, vff_ansatz)\n",
    "    print(f\"Initial cost: {initial_cost:.8f}\")\n",
    "    \n",
    "    # Optimize with gradient descent\n",
    "    print(\"\\nStarting gradient optimization...\")\n",
    "    best_params = run_vff_optimization_simple(target_unitary, initial_params)\n",
    "    \n",
    "    # Evaluate final cost\n",
    "    final_circuit = build_vff_ansatz(best_params)\n",
    "    final_cost = calculate_lhst_cost(target_unitary, final_circuit)\n",
    "    print(f\"Final cost: {final_cost:.8f}\")\n",
    "    \n",
    "    # Evaluate fast-forwarding with best parameters\n",
    "    print(\"\\nEvaluating fast-forwarding performance...\")\n",
    "    ff_results = run_fast_forwarding_evaluation(best_params, target_unitary)\n",
    "    \n",
    "    total_end = datetime.datetime.now()\n",
    "    total_time = (total_end - total_start).total_seconds()\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VFF WORKFLOW SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total runtime: {total_time:.2f} seconds\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"- Qubits: {Config.QUBITS_NUM}\")\n",
    "    print(f\"- Timestep: {Config.timestep}\")\n",
    "    print(f\"- Layers: {Config.NUM_LAYERS_EIGENVECTOR} eigenvector, {Config.NUM_LAYERS_DIAGONAL} diagonal\")\n",
    "    print(f\"\\nFinal Cost: {final_cost:.8f}\")\n",
    "    \n",
    "    print(\"\\nFast-forwarding results:\")\n",
    "    for n, fidelity in ff_results:\n",
    "        print(f\"N={n}: Fidelity={fidelity:.8f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'final_cost': final_cost,\n",
    "        'ff_results': ff_results,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "\n",
    "def run_vff_optimization_simple(target_unitary, initial_params):\n",
    "    total_w_params, total_d_params = Config.calculate_parameter_counts()\n",
    "    total_params = total_w_params + total_d_params\n",
    "    \n",
    "    # Optimization parameters\n",
    "    params = initial_params.copy()\n",
    "    learning_rate = 5.0\n",
    "    best_cost = float('inf')\n",
    "    best_params = params.copy()\n",
    "    \n",
    "    # Create optimization log file\n",
    "    with open(Config.get_optimizer_filename(), 'w') as f:\n",
    "        f.write(\"Iteration\\tCost\\tLearning_Rate\\n\")\n",
    "    \n",
    "    print(\"Starting optimization loop...\")\n",
    "    \n",
    "    for iteration in range(Config.MAX_ITERATIONS):\n",
    "        # Calculate current cost\n",
    "        v_circuit = build_vff_ansatz(params)\n",
    "        cost = calculate_lhst_cost(target_unitary, v_circuit)\n",
    "        \n",
    "        # Log progress\n",
    "        with open(Config.get_optimizer_filename(), 'a') as f:\n",
    "            f.write(f\"{iteration}\\t{cost:.8f}\\t{learning_rate:.6f}\\n\")\n",
    "        \n",
    "        # Update best parameters\n",
    "        if cost < best_cost:\n",
    "            improvement = best_cost - cost\n",
    "            best_cost = cost\n",
    "            best_params = params.copy()\n",
    "            print(f\"Iteration {iteration}: New best cost = {best_cost:.8f} (improvement: {improvement:.8f})\")\n",
    "        else:\n",
    "            print(f\"Iteration {iteration}: Cost = {cost:.8f}\")\n",
    "        \n",
    "        # Calculate gradients\n",
    "        param_indices = list(range(total_params))\n",
    "        gradients = calculate_gradients_parallel(params, target_unitary, param_indices)\n",
    "        \n",
    "        # Check gradient norm\n",
    "        grad_norm = np.linalg.norm(gradients)\n",
    "        print(f\"  Gradient norm: {grad_norm:.6f}\")\n",
    "        \n",
    "        # Update parameters according to equations 31-32\n",
    "        params = params - learning_rate * gradients\n",
    "        params = np.mod(params, 2*np.pi)  # Keep in [0, 2Ï€)\n",
    "        \n",
    "        # Adaptive learning rate\n",
    "        if iteration % 10 == 0 and iteration > 0:\n",
    "            learning_rate *= 0.95  # Gradually reduce learning rate\n",
    "            print(f\"  Reducing learning rate to {learning_rate:.6f}\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    np.savetxt(Config.get_params_filename(), best_params)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Config.QUBITS_NUM = 8\n",
    "    \n",
    "    # Run the optimization workflow\n",
    "    results = run_multi_init_optimization()\n",
    "    \n",
    "    # Save final results\n",
    "    print(\"\\nSaving final results...\")\n",
    "    np.savetxt(Config.get_params_filename(), results['best_params'])\n",
    "    print(f\"Best parameters saved to {Config.get_params_filename()}\")\n",
    "    \n",
    "    print(\"\\nOptimization complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
